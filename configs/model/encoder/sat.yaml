_target_: models.encoders.sat.SemanticAttentionTransformerEncoder
num_input_channels: 3
positional_embedding_dim: 40
num_labels: ${dataset.num_labels}
num_latent_per_labels: 8
num_latents_bg: 8
latent_dim: 256
type_of_initial_latents: learned
attention_latent_dim: 256
num_blocks: 7
num_cross_heads: 1
num_self_heads: 1
image_conv: True
reverse_conv: False
conv_features_dim_first: 16
use_self_attention: True
use_equalized_lr: False
lr_mul: 1.0
use_vae: False